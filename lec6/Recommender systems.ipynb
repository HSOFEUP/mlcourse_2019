{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems on the Movielens dataset\n",
    "\n",
    "A recommender system is a machine learning algorithm that seeks to predict the rating a user would give to an item. One approach to the design of recommender systems that has been widely used is *collaborative filtering*.\n",
    "\n",
    "This problem has become quite popular a few years ago with the *Netflix Prize*: an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings (and without the users or the films being identified except by numbers assigned for the contest -- this is different from so-called content-based methods).\n",
    "\n",
    "First, let us import the usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, dot, add\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Taking a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with the dataset from the Netflix prize, we shall use instead the smaller one from Movielens. Let us load the  Movielens data and take a look at it using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"mldata/ratings.csv\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that user $1$ rated movie $31$ as 2.5 (quite average), but on the other hand likes movie $1172$ quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The number of ratings is 100004. Just for fun, let us read the movie names as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId\n",
       "1                                          Toy Story (1995)\n",
       "2                                            Jumanji (1995)\n",
       "3                                   Grumpier Old Men (1995)\n",
       "4                                  Waiting to Exhale (1995)\n",
       "5                        Father of the Bride Part II (1995)\n",
       "6                                               Heat (1995)\n",
       "7                                            Sabrina (1995)\n",
       "8                                       Tom and Huck (1995)\n",
       "9                                       Sudden Death (1995)\n",
       "10                                         GoldenEye (1995)\n",
       "11                           American President, The (1995)\n",
       "12                       Dracula: Dead and Loving It (1995)\n",
       "13                                             Balto (1995)\n",
       "14                                             Nixon (1995)\n",
       "15                                  Cutthroat Island (1995)\n",
       "16                                            Casino (1995)\n",
       "17                             Sense and Sensibility (1995)\n",
       "18                                        Four Rooms (1995)\n",
       "19                    Ace Ventura: When Nature Calls (1995)\n",
       "20                                       Money Train (1995)\n",
       "21                                        Get Shorty (1995)\n",
       "22                                           Copycat (1995)\n",
       "23                                         Assassins (1995)\n",
       "24                                            Powder (1995)\n",
       "25                                 Leaving Las Vegas (1995)\n",
       "26                                           Othello (1995)\n",
       "27                                      Now and Then (1995)\n",
       "28                                        Persuasion (1995)\n",
       "29        City of Lost Children, The (Cit√© des enfants p...\n",
       "30        Shanghai Triad (Yao a yao yao dao waipo qiao) ...\n",
       "                                ...                        \n",
       "159690    Teenage Mutant Ninja Turtles: Out of the Shado...\n",
       "159755            Popstar: Never Stop Never Stopping (2016)\n",
       "159858                               The Conjuring 2 (2016)\n",
       "159972                       Approaching the Unknown (2016)\n",
       "160080                                  Ghostbusters (2016)\n",
       "160271                          Central Intelligence (2016)\n",
       "160438                                  Jason Bourne (2016)\n",
       "160440                               The Maid's Room (2014)\n",
       "160563                          The Legend of Tarzan (2016)\n",
       "160565                      The Purge: Election Year (2016)\n",
       "160567                Mike & Dave Need Wedding Dates (2016)\n",
       "160590                           Survive and Advance (2013)\n",
       "160656                                      Tallulah (2016)\n",
       "160718                                         Piper (2016)\n",
       "160954                                         Nerve (2016)\n",
       "161084                         My Friend Rockefeller (2015)\n",
       "161155                                     Sunspring (2016)\n",
       "161336                    Author: The JT LeRoy Story (2016)\n",
       "161582                            Hell or High Water (2016)\n",
       "161594                 Kingsglaive: Final Fantasy XV (2016)\n",
       "161830                                          Body (2015)\n",
       "161918                  Sharknado 4: The 4th Awakens (2016)\n",
       "161944                The Last Brickmaker in America (2001)\n",
       "162376                                      Stranger Things\n",
       "162542                                        Rustom (2016)\n",
       "162672                                  Mohenjo Daro (2016)\n",
       "163056                                 Shin Godzilla (2016)\n",
       "163949    The Beatles: Eight Days a Week - The Touring Y...\n",
       "164977                             The Gay Desperado (1936)\n",
       "164979                                Women of '69, Unboxed\n",
       "Name: title, Length: 9125, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_names = pd.read_csv(\"mldata/movies.csv\").set_index('movieId')['title']\n",
    "movie_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now preprocess our data. We will update the user and movie indices so that they are contiguous integers -- this will simplify our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min/max user indices: 0, 670\n",
      "min/max movie indices: 0, 9065\n"
     ]
    }
   ],
   "source": [
    "# Extract unique user and movie ids\n",
    "users = ratings.userId.unique()\n",
    "movies = ratings.movieId.unique()\n",
    "\n",
    "# Create dictionaries mapping index to integer between 1 and n_users/n_movies\n",
    "user_id2idx = {o:i for i, o in enumerate(users)}\n",
    "movie_id2idx = {o:i for i, o in enumerate(movies)}\n",
    "\n",
    "# Update indices \n",
    "ratings.movieId = ratings.movieId.apply(lambda x: movie_id2idx[x])\n",
    "ratings.userId = ratings.userId.apply(lambda x: user_id2idx[x])\n",
    "\n",
    "# Let us check what is the minimum and maximum indices now\n",
    "print(\"min/max user indices: %g, %g\" % (ratings.userId.min(), ratings.userId.max()))\n",
    "print(\"min/max movie indices: %g, %g\" % (ratings.movieId.min(), ratings.movieId.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users/movies: 671, 9066\n"
     ]
    }
   ],
   "source": [
    "n_users = ratings.userId.nunique()\n",
    "n_movies = ratings.movieId.nunique()\n",
    "print(\"number of users/movies: %g, %g\" % (n_users, n_movies)) # (this should give the same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, as we usually do, let us split our data into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask = np.random.rand(len(ratings)) < 0.8\n",
    "train, val = ratings[mask], ratings[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Let us look at a subset of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just to get some intuition on our data, let's look at a subset of the matrix with the 15 users/movies who have the most votes associated to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId\n",
       "546    2391\n",
       "563    1868\n",
       "623    1735\n",
       "14     1700\n",
       "72     1610\n",
       "451    1340\n",
       "467    1291\n",
       "379    1063\n",
       "310    1019\n",
       "29     1011\n",
       "293     947\n",
       "508     923\n",
       "579     922\n",
       "212     910\n",
       "211     876\n",
       "Name: votes, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create series containing vote count per user, then sort it and pick the first 15\n",
    "users_votecount = ratings.groupby('userId')['rating'].count()\n",
    "top_users = users_votecount.sort_values(ascending=False)[:15]\n",
    "top_users.rename(\"votes\", inplace=True)\n",
    "top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId\n",
       "57     341\n",
       "49     324\n",
       "99     311\n",
       "92     304\n",
       "143    291\n",
       "72     274\n",
       "402    259\n",
       "417    247\n",
       "79     244\n",
       "89     237\n",
       "179    234\n",
       "27     228\n",
       "197    226\n",
       "505    224\n",
       "180    220\n",
       "Name: votes, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create series containing vote count per movie, then sort it and pick the first 15\n",
    "movies_votecount = ratings.groupby('movieId')['rating'].count()\n",
    "top_movies = movies_votecount.sort_values(ascending=False)[:15]\n",
    "top_movies.rename(\"votes\", inplace=True)\n",
    "top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>27</th>\n",
       "      <th>49</th>\n",
       "      <th>57</th>\n",
       "      <th>72</th>\n",
       "      <th>79</th>\n",
       "      <th>89</th>\n",
       "      <th>92</th>\n",
       "      <th>99</th>\n",
       "      <th>143</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>197</th>\n",
       "      <th>402</th>\n",
       "      <th>417</th>\n",
       "      <th>505</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  27   49   57   72   79   89   92   99   143  179  180  197  402  417  \\\n",
       "userId                                                                          \n",
       "14       3.0  5.0  1.0  3.0  4.0  4.0  5.0  2.0  5.0  5.0  4.0  5.0  5.0  2.0   \n",
       "29       5.0  5.0  5.0  4.0  5.0  4.0  4.0  5.0  4.0  4.0  5.0  5.0  3.0  4.0   \n",
       "72       4.0  5.0  5.0  4.0  5.0  3.0  4.5  5.0  4.5  5.0  5.0  5.0  4.5  5.0   \n",
       "211      5.0  4.0  4.0  3.0  5.0  3.0  4.0  4.5  4.0  NaN  3.0  3.0  5.0  3.0   \n",
       "212      2.5  NaN  2.0  5.0  NaN  4.0  2.5  NaN  5.0  5.0  3.0  3.0  4.0  3.0   \n",
       "293      3.0  NaN  4.0  4.0  4.0  3.0  NaN  3.0  4.0  4.0  4.5  4.0  4.5  4.0   \n",
       "310      3.0  3.0  5.0  4.5  5.0  4.5  2.0  4.5  4.0  3.0  4.5  4.5  4.0  3.0   \n",
       "379      5.0  5.0  5.0  4.0  NaN  4.0  5.0  4.0  4.0  4.0  NaN  3.0  5.0  4.0   \n",
       "451      4.0  5.0  4.0  5.0  4.0  4.0  5.0  5.0  4.0  4.0  4.0  4.0  2.0  3.5   \n",
       "467      3.0  3.5  3.0  2.5  NaN  NaN  3.0  3.5  3.5  3.0  3.5  3.0  3.0  4.0   \n",
       "508      5.0  5.0  4.0  3.0  5.0  2.0  4.0  4.0  5.0  5.0  5.0  3.0  4.5  3.0   \n",
       "546      NaN  5.0  2.0  3.0  5.0  NaN  5.0  5.0  NaN  2.5  2.0  3.5  3.5  3.5   \n",
       "563      1.0  5.0  3.0  5.0  4.0  5.0  5.0  NaN  2.0  5.0  5.0  3.0  3.0  4.0   \n",
       "579      4.5  4.5  3.5  3.0  4.0  4.5  4.0  4.0  4.0  4.0  3.5  3.0  4.5  4.0   \n",
       "623      NaN  5.0  3.0  3.0  NaN  3.0  5.0  NaN  5.0  5.0  5.0  5.0  2.0  5.0   \n",
       "\n",
       "movieId  505  \n",
       "userId        \n",
       "14       5.0  \n",
       "29       5.0  \n",
       "72       4.0  \n",
       "211      NaN  \n",
       "212      2.0  \n",
       "293      NaN  \n",
       "310      4.0  \n",
       "379      4.0  \n",
       "451      5.0  \n",
       "467      4.0  \n",
       "508      4.5  \n",
       "546      5.0  \n",
       "563      5.0  \n",
       "579      4.5  \n",
       "623      4.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data frame with only top movies/users, and then a cross-tab\n",
    "top_ratings = ratings.loc[ratings[\"movieId\"].isin(top_movies.index) & ratings[\"userId\"].isin(top_users.index)]\n",
    "pd.crosstab(top_ratings[\"userId\"], top_ratings[\"movieId\"], top_ratings[\"rating\"], aggfunc=sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Matrix factorization approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One of the simplest models for recommender systems, which has been (and still is) a very popular one, is that of low-rank matrix factorization.\n",
    "\n",
    "In this model, one assumes the presence of *latent factors* for each movie and user, that will generate the final rating.\n",
    "More precisely, we look for a factorization of the true rating matrix, i.e. we are trying to find two matrices $U$ and $V$ such that the true rating matrix $Y$ is represented as\n",
    "\n",
    "$$Y=UV^T + Z\n",
    "\\qquad\\text{or, in components,}\\qquad\n",
    "Y_{ia} = \\vec U_i \\cdot \\vec V_a + Z_{ia} = \\sum_p U_{ip} V_{ap} + Z_{ia}$$\n",
    "\n",
    "Here we label users by $i$ and movies by $a$.\n",
    "The matrix $Z$ will be interpreted as a Gaussian noise term:\n",
    "then, training amounts to minimize the mean square error\n",
    "\n",
    "$$MSE = \\sum_{(ia)\\in {\\cal T}} (Y_{ia} - (U V^T)_{ia} )^2$$\n",
    "\n",
    "using the pairs $(ia)\\in {\\cal T}$ in the training set. Once the matrices have been trained, \n",
    "we can use $Y_{ia} = \\sum_p U_{ip} V_{ap}$ to predict the unknown rating of movie $a$ by user $i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture and Keras implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, Keras/Tensorflow can also solve problems like this! Let's see how well that works. \n",
    "We are not going to use a `Sequential()` model anymore, but instead the more general `Model()`. We will employ four different types of layers: `Input` and `Embedding` layers for the users and for the movies, and `Dot` and `Flatten` layers combining them.\n",
    "\n",
    "Assuming that $U$ and $V$ have rank $50$ (so that we have $50$ latent variables for each users/movies), we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        33550       user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 50)        453300      movie[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1)            0           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 486,850\n",
      "Trainable params: 486,850\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_factors = 50\n",
    "\n",
    "# Specify model in Keras using embedding layers\n",
    "user_input = Input(shape=(1,), dtype='int64', name='user')\n",
    "U = Embedding(n_users, n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(user_input)\n",
    "movie_input = Input(shape=(1,), dtype='int64', name='movie')\n",
    "V = Embedding(n_movies, n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(movie_input)\n",
    "Y = dot([U, V], axes=-1)\n",
    "Y_r = Flatten()(Y)\n",
    "\n",
    "model = Model([user_input, movie_input], Y_r)\n",
    "model.compile(adam(lr=1e-3), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80099 samples, validate on 19905 samples\n",
      "Epoch 1/5\n",
      "80099/80099 [==============================] - 9s 111us/step - loss: 10.1458 - val_loss: 4.4030\n",
      "Epoch 2/5\n",
      "80099/80099 [==============================] - 9s 110us/step - loss: 3.1957 - val_loss: 2.8301\n",
      "Epoch 3/5\n",
      "80099/80099 [==============================] - 9s 108us/step - loss: 2.4148 - val_loss: 2.6164\n",
      "Epoch 4/5\n",
      "80099/80099 [==============================] - 9s 109us/step - loss: 2.2318 - val_loss: 2.5680\n",
      "Epoch 5/5\n",
      "80099/80099 [==============================] - 9s 110us/step - loss: 2.1599 - val_loss: 2.5541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb29a88f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=5\n",
    "batch_size = 64\n",
    "\n",
    "# Train model\n",
    "model.fit([train.userId, train.movieId], train.rating, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the learning rate before we start overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80099 samples, validate on 19905 samples\n",
      "Epoch 1/3\n",
      "80099/80099 [==============================] - 8s 102us/step - loss: 2.1197 - val_loss: 2.5612\n",
      "Epoch 2/3\n",
      "80099/80099 [==============================] - 8s 102us/step - loss: 2.0900 - val_loss: 2.5622\n",
      "Epoch 3/3\n",
      "80099/80099 [==============================] - 9s 107us/step - loss: 2.0669 - val_loss: 2.5679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb27df02e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "epochs=3\n",
    "\n",
    "model.fit([train.userId, train.movieId], train.rating, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still minimizing the training loss, but at this point we are clearly ovefitting since the validation loss is increasing... So our model does not seems so good.  On similar data sets, the [best benchmarks](http://www.librec.net/example.html) are a bit above 0.9, so something is not right here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Adding a bias to users and movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Clearly, we have a problem. We have forgotten that not all users are equal, and not all movies are equal. Some users always rate movies high, other always rates them low: each person has a different reference frame. The same is true for movies: everyone likes The Godfather, so we do not take much risk when we predict someone wants to watch it.\n",
    "\n",
    "This can be fixed by introducing **bias terms** - that is, a different *bias* for each user and each movie, representing how positive or negative user votes typically are, and how good or bad each movie is typically considered to be. \n",
    "\n",
    "To take into account the bias we modify the model as\n",
    "\n",
    "$$Y_{ia} = B_i + B_a + \\vec U_i \\cdot \\vec V_a + Z_{ia}$$\n",
    "\n",
    "We can add that easily by simply creating an additional embedding with one output for each movie and each user, and adding it to our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 50)        33550       user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 50)        453300      movie[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 1)         0           embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 1)         671         user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 1)         9066        movie[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1)            0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 496,587\n",
      "Trainable params: 496,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model as before\n",
    "user_input = Input(shape=(1,), dtype='int64', name='user')\n",
    "U = Embedding(n_users, n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(user_input)\n",
    "movie_input = Input(shape=(1,), dtype='int64', name='movie')\n",
    "V = Embedding(n_movies, n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(movie_input)\n",
    "Y = dot([U, V], axes=-1)\n",
    "Y_r = Flatten()(Y)\n",
    "\n",
    "# Define bias terms and add them to output\n",
    "user_bias = Flatten()(Embedding(n_users, 1, input_length=1)(user_input))\n",
    "movie_bias = Flatten()(Embedding(n_movies, 1, input_length=1)(movie_input))\n",
    "Y_b = add([Y_r, user_bias, movie_bias])\n",
    "\n",
    "# Compile model\n",
    "model = Model([user_input, movie_input], Y_b)\n",
    "model.compile(adam(lr=1e-3), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80099 samples, validate on 19905 samples\n",
      "Epoch 1/10\n",
      "80099/80099 [==============================] - 9s 109us/step - loss: 8.8080 - val_loss: 3.4899\n",
      "Epoch 2/10\n",
      "80099/80099 [==============================] - 9s 110us/step - loss: 2.5869 - val_loss: 2.3053\n",
      "Epoch 3/10\n",
      "80099/80099 [==============================] - 7s 93us/step - loss: 1.9951 - val_loss: 2.1133\n",
      "Epoch 4/10\n",
      "80099/80099 [==============================] - 8s 98us/step - loss: 1.8328 - val_loss: 2.0188\n",
      "Epoch 5/10\n",
      "80099/80099 [==============================] - 8s 96us/step - loss: 1.7372 - val_loss: 1.9406\n",
      "Epoch 6/10\n",
      "80099/80099 [==============================] - 8s 101us/step - loss: 1.6564 - val_loss: 1.8739\n",
      "Epoch 7/10\n",
      "80099/80099 [==============================] - 9s 109us/step - loss: 1.5799 - val_loss: 1.8087\n",
      "Epoch 8/10\n",
      "80099/80099 [==============================] - 9s 107us/step - loss: 1.5074 - val_loss: 1.7412\n",
      "Epoch 9/10\n",
      "80099/80099 [==============================] - 9s 107us/step - loss: 1.4369 - val_loss: 1.6795\n",
      "Epoch 10/10\n",
      "80099/80099 [==============================] - 9s 107us/step - loss: 1.3696 - val_loss: 1.6193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb29a7f518>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit([train.userId, train.movieId], train.rating, batch_size=64, epochs=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try using smaller and smaller learning rates until we see we are overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit([train.userId, train.movieId], train.rating, batch_size=64, epochs=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 5e-5\n",
    "model.fit([train.userId, train.movieId], train.rating, batch_size=64, epochs=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit([train.userId, train.movieId], train.rating, batch_size=64, epochs=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Seems like we are overfitting now... Still this result is quite respectable: our validation loss is about 1, so our error is around one of the benchmarks we could find with a quick Google search. So this looks like a great approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"mldata/bias.h5\")\n",
    "model.load_weights(\"mldata/bias.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user #3 would really enjoy movie #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict([np.array([3]), np.array([6])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but not movie 100..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([np.array([3]), np.array([100])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually makes sense, if you ask me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_names[movies[6]], \" \", movie_names[movies[100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Analyze and interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To make the analysis of the factors more interesting, we shall restrict it to the top 2000 most popular movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "movies_votecount = ratings.groupby('movieId')['rating'].count()\n",
    "top_movies = movies_votecount.sort_values(ascending=False)[:2000]\n",
    "top_movies.rename(\"votes\", inplace=True)\n",
    "\n",
    "# Let us create at data frame and handle all data together\n",
    "top_movies = pd.DataFrame(top_movies)\n",
    "top_movies[\"title\"] = movie_names[movies[top_movies.index]].values\n",
    "top_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we'll look at the movie bias term. We create a \"model\" - which in Keras is simply a way of associating one or more inputs with one or more outputs, using the functional API. Here, our input is the movie id (a single id), and the output is the movie bias (a single float).\n",
    "\n",
    "We can then look at the top and bottom rated movies. These ratings are corrected for different levels of reviewer sentiment, as well as different types of movies that different reviewers watch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_movie_bias = Model(movie_input, movie_bias)\n",
    "top_movies[\"bias\"] = get_movie_bias.predict(top_movies.index)\n",
    "top_movies.sort_values(\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are indeed quite bad if you ask me..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_movies.sort_values(\"bias\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and those are quite good ones! We can now do the same thing at the level of the embeddings to see what are the *features* that have been learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_movie_embed = Model(movie_input, V)\n",
    "movie_embed = np.squeeze(get_movie_embed.predict([top_movies.index]))\n",
    "movie_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Because it's hard to interpret $50$ embeddings, we use [PCA](https://plot.ly/ipython-notebooks/principal-component-analysis/) to simplify them down to just $3$ vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "movie_pca = pca.fit(movie_embed.T).components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's the 1st component. It seems to be related to how \"critically acclaimed\" or \"classic\" the movie is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_movies[\"pca_1\"] = movie_pca[0]\n",
    "top_movies.sort_values(\"pca_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The 2nd looks like \"blockbuster\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_movies[\"pca_2\"] = movie_pca[1]\n",
    "top_movies.sort_values(\"pca_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The 3rd seems to be looking at violent vs. happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_movies[\"pca_3\"] = movie_pca[2]\n",
    "top_movies.sort_values(\"pca_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can draw a picture to see how various movies appear on the map of these PCA components. This picture will show us the 1st and 3rd components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 30\n",
    "\n",
    "xx = movie_pca[0, start:end]\n",
    "yy = movie_pca[2, start:end]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(xx, yy)\n",
    "for title, x, y in zip(top_movies.iloc[start:end][\"title\"].values, xx, yy):\n",
    "    plt.text(x + 1e-3, y + 1e-3, title, color=\"k\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Moving to neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tried to be clever, using the kind of techniques that were used for the Netflix prize, we should try to blindly use generic powerful techniques, like... a neural network! Indeed, rather than creating a *special purpose* architecture (like our matrix factorization with bias), it's often both easier and more accurate to use a standard neural network.\n",
    "\n",
    "Let's try it! Here, we simply concatenate the user and movie embeddings into a single vector, which we feed into the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Dense, Dropout\n",
    "\n",
    "# Generate embeddings and concatenate them\n",
    "user_input = Input(shape=(1,), dtype='int64', name='user')\n",
    "U = Embedding(n_users, n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(user_input)\n",
    "movie_input = Input(shape=(1,), dtype='int64', name='movie')\n",
    "V = Embedding(n_movies, n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(movie_input)\n",
    "Y = concatenate([U, V])\n",
    "Y_r = Flatten()(Y)\n",
    "\n",
    "# Specify neural network architecture\n",
    "Y_nn = Dropout(0.3)(Y_r)\n",
    "Y_nn = Dense(70, activation='relu')(Y_nn)\n",
    "Y_nn = Dropout(0.75)(Y_nn)\n",
    "Y_nn = Dense(1)(Y_nn)\n",
    "nn = Model([user_input, movie_input], Y_nn)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "nn.compile(adam(0.001), loss='mse')\n",
    "nn.fit([train.userId, train.movieId], train.rating, batch_size=64, epochs=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! This improves on our accuracy even further right away! The power of neural nets is definitely impressive! At this point you can try to play with the architecture and see if you can improve on it again.\n",
    "\n",
    "Note it is a bit harder to interpret these results: we wouldn't be able to do an analysis like the one we did in the last section. This is one the big issues in deep learning today: *interpretability*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
