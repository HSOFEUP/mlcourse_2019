{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on MNIST using the MultiLayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the MNIST data. Differently from before, we are not going to use scikitlearn to do that, but instead the Keras library. That's because we are going to move from \"classic\" methods to so-called *deep learning*, i.e. machine learning using deep neural networks.\n",
    "\n",
    "Keras is built on top of Tensorflow, a framework for deep learning developed by Google which is probably the one most used by practitioners (PyTorch, developed at Facebook, is an alternative that has been gaining traction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 18s 2us/step\n",
      "no. of training/test samples: 60000, 10000\n",
      "no. of features: 784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"no. of training/test samples: %d, %d\" % (X_train.shape[0], X_test.shape[0]))\n",
    "print(\"no. of features: %d\" % (np.product(X_train.shape[1:])))\n",
    "\n",
    "# Let us look at some of the digits\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for d in range(4):\n",
    "    axs[d % 2, d // 2].imshow(X_train[d], cmap=\"gray\")\n",
    "    axs[d % 2, d // 2].set_title(\"correct label: %d\" % (y_train[d]))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Keras loads MNIST in a slightly different way than scikitlearn: the data is already shuffled and split in training/test sets; moreover, each sample is a 2D array of dimension 28x28, instead of a 1D array of dimension 784.\n",
    "\n",
    "We proceed by doing some preprocessing. We will *rasterize* these vectors, i.e. transform them from two-dimensional to one-dimensional arrays; moreover, we will normalize them, so that each component of the array is valued between 0 and 1, instead of 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories of the images above is now encoded as: \n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Rasterize and normalize samples\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Use 32-bit instead of 64-bit float\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "# Work with one-hot encoding of labels\n",
    "n_classes = len(np.unique(y_train))\n",
    "y_train_b = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test_b = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print(\"The categories of the images above is now encoded as: \\n\", y_train_b[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to build our multi-layer neural network. First we shall do it with a single hidden layer.\n",
    "\n",
    "In the code below we can see how it's easy to set up models in Keras. With just a few lines we have built a 2-layer neural network, and generalizing from this to more layers is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specify model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation=\"relu\", input_shape=(784,)))\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "# Print model and compile it\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first dense layer has $784\\times 512 + 512$ parameters, the second has $512 \\times 10 + 10$. They correspond to the weights, plus a bias applied to each output.\n",
    "We compile the model using the categorical cross entropy loss, and the RMSprop algorithm for optimization, described [here](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop) together with other variants of gradient descent.\n",
    "\n",
    "After setting up the model, we can train it just like we do in scikitlearn: by doing `model.fit(X_train, y_train)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2600 - acc: 0.9250 - val_loss: 0.1614 - val_acc: 0.9485\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.1085 - acc: 0.9679 - val_loss: 0.1048 - val_acc: 0.9672\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0717 - acc: 0.9792 - val_loss: 0.0925 - val_acc: 0.9715\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0515 - acc: 0.9853 - val_loss: 0.0844 - val_acc: 0.9741\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0379 - acc: 0.9899 - val_loss: 0.0850 - val_acc: 0.9752\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0279 - acc: 0.9927 - val_loss: 0.0812 - val_acc: 0.9778\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0204 - acc: 0.9949 - val_loss: 0.0824 - val_acc: 0.9782\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0846 - val_acc: 0.9779\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0111 - acc: 0.9975 - val_loss: 0.0871 - val_acc: 0.9784\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0915 - val_acc: 0.9778\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.0889 - val_acc: 0.9801\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0861 - val_acc: 0.9810\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.1005 - val_acc: 0.9779\n",
      "Epoch 14/20\n",
      "17920/60000 [=======>......................] - ETA: 3s - loss: 0.0028 - acc: 0.9994"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Perform fit\n",
    "history = model.fit(X_train, y_train_b,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=False,\n",
    "                    validation_data=(X_test, y_test_b))\n",
    "\n",
    "\n",
    "# Print results\n",
    "score = model.evaluate(X_test, y_test_b, verbose=0)\n",
    "print('Test loss/accuracy: %g, %g' % (score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5)) \n",
    "# Plot history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy -- MLP')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss -- MLP')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually pretty good, an accuracy of 2% was the state of the art for a long time (check it [here](http://yann.lecun.com/exdb/mnist/)). \n",
    "\n",
    "Just out of curiosity, [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html) is a detailed list of the best results people have been able to obtain on MNIST. The smallest test error achieved so far is of 0.21%, i.e. only 21 of the 10000 test samples are not classified correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Playing a bit with our MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many degrees of freedom both in our model and in the training procedure, that have been chosen more or less arbitrarily. For instance, why do we use these values for `batch_size` and `epochs`? Why do we use a ReLU activation on the 1st layer, and a softmax activation on the second? Why do we use this loss, this optimizer etc.\n",
    "\n",
    "This is one of the main problem with deep learning: it is not clear in principle what approach is going to give the best result, and there are *many* possible approaches. So we just need to try many things and see what happens. \n",
    "\n",
    "We can do cross-validation, but the possibilities are so many that trying everything is just not feasible. So we need to develop a gut feeling on what's more appropriate for each situation -- sometimes theory can guide us, sometimes it can't.\n",
    "\n",
    "We will now see how things change if we add dropout and batch normalization, take a more shallow network, or add a second layer. It seems however to be difficult to improve too much on the 2% performance with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout and batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the numbers printed on screen during training, we can see that our model was actually overfitting a lot: we get essentially 100% accuracy on the training set!\n",
    "\n",
    "There are some tricks that have been introduced in order to handle this overfitting. The first one of them is called *dropout* (introduced [here](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)), and consists in, at each step of the training procedure, ignoring some of the features. This is more or less what we did when we looked at random forests: each of the trees in the forest worked with a subset of the features. So we can see dropout as a sort of bagging procedure which makes the model more robust. The difference is that dropout can be applied to each layer, not only to the first input layer.\n",
    "\n",
    "The other trick is *batch normalization*, which consists in taking each sample in a mini-batch and standardizing it using the *mini-batch* mean and variance, see [here](https://arxiv.org/pdf/1502.03167.pdf) for details.\n",
    "\n",
    "These tricks have been introduced mostly in a ad-hoc way -- people have some explanations on why they work, and have tried to do some theory about it, but in the end we still don't fully understand why and when they work (see [Ali Rahimi's provocative talk](https://www.youtube.com/watch?v=Qi1Yry33TQE))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, BatchNormalization, Activation\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Specify model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_dim=784, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(n_classes, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Print model and compile it\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Perform fit\n",
    "history_dropout = model.fit(X_train, y_train_b,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test_b))\n",
    "\n",
    "\n",
    "# Print results\n",
    "score = model.evaluate(X_test, y_test_b, verbose=0)\n",
    "print('Test loss/accuracy: %g, %g' % (score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5)) \n",
    "# Plot history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history_dropout.history['acc'])\n",
    "plt.plot(history_dropout.history['val_acc'])\n",
    "plt.title('model accuracy -- MLP with dropout and BN')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history_dropout.history['loss'])\n",
    "plt.plot(history_dropout.history['val_loss'])\n",
    "plt.title('model loss -- MLP with dropout and BN')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying other optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using an optimizer known as RMSprop, which consists in a gradient descent with some extra terms involved. There are however other possibilities. Another optimizer based on gradient descent, Adam, has recently been introduced, and was shown to lead to very good results in practice.\n",
    "\n",
    "Nice comparisons of different optimizers are available [here](http://ruder.io/optimizing-gradient-descent/) and [here](https://3dbabove.com/2017/11/14/optimizationalgorithms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Specify model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_dim=784, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(n_classes, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Print model and compile it\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Perform fit\n",
    "history_adam = model.fit(X_train, y_train_b,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test_b))\n",
    "\n",
    "\n",
    "# Print results\n",
    "score = model.evaluate(X_test, y_test_b, verbose=0)\n",
    "print('Test loss/accuracy: %g, %g' % (score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5)) \n",
    "# Plot history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history_adam.history['acc'])\n",
    "plt.plot(history_adam.history['val_acc'])\n",
    "plt.title('model accuracy -- MLP using Adam')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history_adam.history['loss'])\n",
    "plt.plot(history_adam.history['val_loss'])\n",
    "plt.title('model loss -- MLP using Adam')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we just add another layer? Let's say it is identical to the first one (although it could be anything really -- the possibilities here are infinite!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Specify model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_dim=784, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, input_dim=784, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(n_classes, init=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Print model and compile it\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Perform fit\n",
    "history_onemorelayer = model.fit(X_train, y_train_b,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=False,\n",
    "                    validation_data=(X_test, y_test_b))\n",
    "\n",
    "\n",
    "# Print results\n",
    "score = model.evaluate(X_test, y_test_b, verbose=0)\n",
    "print('Test loss/accuracy: %g, %g' % (score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5)) \n",
    "# Plot history for accuracy\n",
    "plt.subplot(121)\n",
    "plt.plot(history_onemorelayer.history['acc'])\n",
    "plt.plot(history_onemorelayer.history['val_acc'])\n",
    "plt.title('model accuracy -- MLP with three layers')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(history_onemorelayer.history['loss'])\n",
    "plt.plot(history_onemorelayer.history['val_loss'])\n",
    "plt.title('model loss -- MLP with three layers')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's very hard to get past the 98.something% test accuracy. In order to do that we'll have to look into even more complicated models -- more specifically, layers other than the dense one. We'll do that on next class!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
