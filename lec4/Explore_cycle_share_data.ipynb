{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore cycle share data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Installation requirements\n",
    "\n",
    "You must install the following packages to follow this notebook:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge altair vega_datasets vega\n",
    "conda install -c conda-forge ipyleaflet\n",
    "pip install dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "The data was downloaded from the [Kaggle Datasets](https://www.kaggle.com/pronto/cycle-share-dataset) repository,\n",
    "and was originally provided by Pronto, the company that operates the cycle share system at Seattle, as part of an open data initiative.\n",
    "\n",
    "On the Kaggle repository you will find a detailed data description, reproduced here for the reader convenience.\n",
    "## Context\n",
    "\n",
    "The Pronto Cycle Share system consists of 500 bikes and 54 stations located in Seattle. Pronto provides open data on individual trips, stations, and daily weather.\n",
    "\n",
    "## Content\n",
    "\n",
    "There are 3 datasets that provide data on the stations, trips, and weather from 2014-2016.\n",
    "\n",
    "1. Station dataset\n",
    "    \n",
    "    - station_id: station ID number\n",
    "    - name: name of station\n",
    "    - lat: station latitude\n",
    "    - long: station longitude\n",
    "    - install_date: date that station was placed in service\n",
    "    - install_dockcount: number of docks at each station on the installation date\n",
    "    - modification_date: date that station was modified, resulting in a change in location or dock count\n",
    "    - current_dockcount: number of docks at each station on 8/31/2016\n",
    "    - decommission_date: date that station was placed out of service\n",
    "\n",
    "\n",
    "2. Trip dataset\n",
    "    \n",
    "    - trip_id: numeric ID of bike trip taken\n",
    "    - starttime: day and time trip started, in PST\n",
    "    - stoptime: day and time trip ended, in PST\n",
    "    - bike_id: ID attached to each bike\n",
    "    - tripduration: time of trip in seconds\n",
    "    - from_station_name: name of station where trip originated\n",
    "    - to_station_name: name of station where trip terminated\n",
    "    - from_station_id: ID of station where trip originated\n",
    "    - to_station_id: ID of station where trip terminated\n",
    "    - usertype: \"Short-Term Pass Holder\" is a rider who purchased a 24-Hour or 3-Day Pass; \"Member\" is a rider who purchased a Monthly or an Annual Membership\n",
    "    - gender: gender of rider\n",
    "    - birthyear: birth year of rider\n",
    "\n",
    "\n",
    "3. Weather dataset contains daily weather information in the service area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv data in pandas \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"cycle-share-dataset\"\n",
    "\n",
    "station = pd.read_csv(\n",
    "    os.path.join(data_dir, \"station.csv\"), \n",
    "    parse_dates=[\"install_date\",\"modification_date\",\"decommission_date\"]\n",
    ")\n",
    "\n",
    "trip = pd.read_csv(\n",
    "    os.path.join(data_dir, \"trip.csv\"), \n",
    "    parse_dates=[\"starttime\",\"stoptime\"],\n",
    "    skiprows=range(1,50794) # the first 50794 are duplicates ...\n",
    ")\n",
    "assert trip[\"trip_id\"].nunique()==trip.shape[0]\n",
    "\n",
    "weather = pd.read_csv(\n",
    "    os.path.join(data_dir, \"weather.csv\"), \n",
    "    parse_dates=[\"Date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n",
    "\n",
    "Before using any data you should check it thoroughly ! Let's check for instance the `station` dataset. The data description already provides a lot of information:\n",
    "\n",
    "- the meaning (and therefore expected data-type) of each column\n",
    "- there are 54 stations, uniquely identified by `station_id`\n",
    "\n",
    "First let's see how many rows we have and list the columns along with their data-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"station: {station.shape[0]} rows {station.shape[1]} columns\\n\")\n",
    "print(station.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 58 records and not 54. Are there any duplicated rows ? Let's how many distinct `station_id` we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_station_id = station[\"station_id\"].nunique()\n",
    "print(f\"There are {n_station_id} distinct station_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have no duplicate and exactly one record per `station_id`. There are in fact 58 stations in our dataset, not 54. \n",
    "\n",
    "Now let's take a look at the first rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all fields are consistent with the description: `lat` and `long` look like latitutde and longitude, `name` as a station name, the counts look like counts and dates like dates.\n",
    "\n",
    "However the `modification_date` and `decommission_date` are all missing in the first 5 rows: values are all `NaT` meaning Not-a-Time. For the dates let's see how many missing values we have, and also the min and max date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_column in [\"install_date\", \"modification_date\", \"decommission_date\"]:\n",
    "    n_missing = station[date_column].isnull().sum()\n",
    "    date_min = station[date_column].min()\n",
    "    date_max = station[date_column].max()\n",
    "    print(f\"{date_column}: {n_missing} are missing min={date_min} max={date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense: all have an installation date, a few have been modified and only 58 - 54 = 4 were decommissioned.\n",
    "Probably the 54 in the data description was referring to stations still in service.\n",
    "\n",
    "We can list here the 4 out of service stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[station.decommission_date.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the remaining numeric columns `lat`, `long`, `install_dockcount` and `current_dockcount`, does the range of values make sense ? Are their any missing values ? Let's use the `describe()` method to get a quick statistical summary for each column, and `.T` to transpose the summary stats dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are no missing values (**count** counts the number of non-null values), and every range of values makes sense. For instance, Seattle is located at $47^{\\circ}N-122^{\\circ}W$.\n",
    "\n",
    "Now let's check the `trip` dataset. If the start and stop time are consitent we expect that \n",
    "\n",
    "- `stoptime` $>$ `startime`,\n",
    "- `tripduration` $\\simeq$ `stoptime` $-$ `startime` (in seconds).\n",
    "\n",
    "But actually there are a few errors !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"trip: {trip.shape[0]} records\")\n",
    "# count number of trips where stoptime < starttime\n",
    "n_time_travel = (trip[\"stoptime\"] < trip[\"starttime\"]).sum()\n",
    "print(f\"The {n_time_travel} trips for which stoptime < startime:\")\n",
    "# show the few outliers\n",
    "trip.query(\"stoptime < starttime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recompute the trip duration in seconds from stop and start time:\n",
    "trip[\"computed_duration\"] = (trip[\"stoptime\"]-trip[\"starttime\"]).dt.seconds\n",
    "# and see if there is more than a 1min = 60s difference with tripduration:\n",
    "trip[\"over_1min\"] = (trip[\"computed_duration\"]-trip[\"tripduration\"]).abs() > 60\n",
    "# show the few outliers\n",
    "print(\"Trips with over 1min difference between stoptime-starttime and tripduration\")\n",
    "# we focus on the stoptime > startime trips, the stoptime < startime trips are shown just above\n",
    "trip.query(\"(stoptime > starttime) & over_1min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually very few errors (9 out of 236065). Let's filter out the bad rows, and drop the utility columns\n",
    "`computed_duration` and `over_1min` we have created for the sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilde is the logical NOT operator\n",
    "trip = trip.query(\"(stoptime > starttime) & ~over_1min\")\n",
    "# dropping columns\n",
    "trip = trip.drop(\n",
    "    columns=[\"computed_duration\", \"over_1min\"]\n",
    ")\n",
    "print(f\"Filtered trip dataset: we now have {trip.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one final check. You may have notice that the trip dataset contains the ID of the stations where the trip originated and terminated. But do we recover every `station_id` of the `trip` dataset in the `station` dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = station[\"station_id\"].unique().tolist()\n",
    "from_station_ids = trip[\"from_station_id\"].unique().tolist()\n",
    "to_station_ids = trip[\"to_station_id\"].unique().tolist()\n",
    "trip_station_ids = set(from_station_ids + to_station_ids)\n",
    "not_in_station = [\n",
    "    station_id for station_id in trip_station_ids\n",
    "    if station_id not in station_ids\n",
    "]\n",
    "print(\n",
    "    f\"{len(not_in_station)} / {len(trip_station_ids)} ids not recovered in station\", \n",
    "    not_in_station\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These station ids indeed seem special, maybe they correspond to a repair / maintetance shop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This dataset is very clean : all fields have a clear meaning that match the data type, no weird values, each row corresponds to exactly one station. Note that such high quality data is more the exception than the rule...\n",
    "\n",
    "Usually data is very messy, and you will spend a considerable amount of time cleaning it.\n",
    "Unfortunately people are relentlessly creative to mess things up: often each dataset is messy in its own unique way.\n",
    "Fields can be incoherent (start time $>$ end time), using $-1$ or $999$ as missing values, aberrant values, duplicated rows, unintelligble column names, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Your turn ! Check that:\n",
    "\n",
    "- each trip corresponds to a single record in the `trip` dataset (no duplicates)\n",
    "- the values for `gender`, `usertype` and `birthyear` make sense\n",
    "- the number of bikes agrees with the data description\n",
    "- the values in `weather` data make sense\n",
    "\n",
    "From wikipedia:\n",
    "- The dew point is the temperature to which air must be cooled to become saturated with water vapor.\n",
    "- A gust or wind gust is a brief increase in the speed of the wind, usually less than 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load exo1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
