{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore cycle share data\n",
    "\n",
    "This notebook is directly inspired by this excellent [blogpost](http://jakevdp.github.io/blog/2015/10/17/analyzing-pronto-cycleshare-data-with-python-and-pandas/) by Jake VanderPlas. Incidentally, Jake VanderPlas is also a lead developer of the [Altair](https://altair-viz.github.io/) package that we will use for data viz and the author of the \n",
    "[Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "The data was downloaded from the [Kaggle Datasets](https://www.kaggle.com/pronto/cycle-share-dataset) repository.\n",
    "It was originally provided by Pronto, the company that operates the cycle share system at Seattle, as part of an open data initiative.\n",
    "You can find the following data description on the Kaggle repository.\n",
    "\n",
    "## Context\n",
    "\n",
    "The Pronto Cycle Share system consists of 500 bikes and 54 stations located in Seattle. Pronto provides open data on individual trips, stations, and daily weather.\n",
    "\n",
    "## Content\n",
    "\n",
    "There are 3 datasets that provide data on the stations, trips, and weather from 2014-2016.\n",
    "\n",
    "1. Station dataset\n",
    "    \n",
    "    - station_id: station ID number\n",
    "    - name: name of station\n",
    "    - lat: station latitude\n",
    "    - long: station longitude\n",
    "    - install_date: date that station was placed in service\n",
    "    - install_dockcount: number of docks at each station on the installation date\n",
    "    - modification_date: date that station was modified, resulting in a change in location or dock count\n",
    "    - current_dockcount: number of docks at each station on 8/31/2016\n",
    "    - decommission_date: date that station was placed out of service\n",
    "\n",
    "\n",
    "2. Trip dataset\n",
    "    \n",
    "    - trip_id: numeric ID of bike trip taken\n",
    "    - start_time: day and time trip started, in PST\n",
    "    - stop_time: day and time trip ended, in PST\n",
    "    - bike_id: ID attached to each bike\n",
    "    - trip_duration: time of trip in seconds\n",
    "    - from_station_name: name of station where trip originated\n",
    "    - to_station_name: name of station where trip terminated\n",
    "    - from_station_id: ID of station where trip originated\n",
    "    - to_station_id: ID of station where trip terminated\n",
    "    - user_type: \"Short-Term Pass Holder\" is a rider who purchased a 24-Hour or 3-Day Pass; \"Member\" is a rider who purchased a Monthly or an Annual Membership\n",
    "    - gender: gender of rider\n",
    "    - birth_year: birth year of rider\n",
    "\n",
    "\n",
    "3. Weather dataset contains daily weather information in the service area\n",
    "\n",
    "    - speeds in miles-per-hour\n",
    "    - temperatures in Fahrenheit\n",
    "    - visibilities in miles\n",
    "    - precipitation in inch\n",
    "    - pressure in inch of mercury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv data in pandas \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"cycle-share-dataset\"\n",
    "\n",
    "station = pd.read_csv(\n",
    "    os.path.join(data_dir, \"station.csv\"), \n",
    "    parse_dates=[\"install_date\",\"modification_date\",\"decommission_date\"]\n",
    ")\n",
    "\n",
    "trip = pd.read_csv(\n",
    "    os.path.join(data_dir, \"trip.csv\"), \n",
    "    parse_dates=[\"start_time\",\"stop_time\"],\n",
    "    infer_datetime_format=True,\n",
    "    skiprows=range(1,50794) # the first 50794 rows are duplicates ...\n",
    ")\n",
    "assert trip[\"trip_id\"].nunique()==trip.shape[0]\n",
    "\n",
    "weather = pd.read_csv(\n",
    "    os.path.join(data_dir, \"weather.csv\"), \n",
    "    parse_dates=[\"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n",
    "\n",
    "Before using any data you should check it thoroughly ! Let's check for instance the `station` dataset. The data description already provides a lot of information:\n",
    "\n",
    "- the meaning (and therefore expected data-type) of each column\n",
    "- there are 54 stations, uniquely identified by `station_id`\n",
    "\n",
    "First let's see how many rows we have and list the columns along with their data-types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"station: {station.shape[0]} rows {station.shape[1]} columns\\n\")\n",
    "print(station.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 58 records and not 54. Are there any duplicated rows ? Let's how many distinct `station_id` we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_station_id = station[\"station_id\"].nunique()\n",
    "print(f\"There are {n_station_id} distinct station_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have no duplicate and exactly one record per `station_id`. There are in fact 58 stations in our dataset, not 54. \n",
    "\n",
    "Now let's take a look at the first rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all fields are consistent with the description: `lat` and `long` look like latitude and longitude, `name` as a station name, the counts look like counts and dates like dates.\n",
    "\n",
    "However the `modification_date` and `decommission_date` are all missing in the first 5 rows: values are all `NaT` meaning Not-a-Time. For the dates let's see how many missing values we have, as well as the min and max date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_column in [\"install_date\", \"modification_date\", \"decommission_date\"]:\n",
    "    n_missing = station[date_column].isnull().sum()\n",
    "    date_min = station[date_column].min()\n",
    "    date_max = station[date_column].max()\n",
    "    print(f\"{date_column}: {n_missing} are missing min={date_min} max={date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense: all have an installation date, a few have been modified and only 58 - 54 = 4 were decommissioned.\n",
    "Probably the 54 in the data description was referring to stations still in service.\n",
    "\n",
    "We can list here the 4 out of service stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[station.decommission_date.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the remaining numeric columns `lat`, `long`, `install_dockcount` and `current_dockcount`, does the range of values make sense ? Are their any missing values ? Let's use the `describe()` method to get a quick statistical summary for each column, and `.T` to transpose the summary stats dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are no missing values (**count** counts the number of non-null values), and every range of values makes sense. For instance, Seattle is located at $47.6^{\\circ}N-122.3^{\\circ}W$.\n",
    "\n",
    "Now let's check the `trip` dataset. If the start and stop time are consitent we expect that \n",
    "\n",
    "- `stop_time` $>$ `start_time`,\n",
    "- `trip_duration` $\\simeq$ `stop_time` $-$ `start_time` (in seconds).\n",
    "\n",
    "But actually there are a few errors !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"trip: {trip.shape[0]} records\")\n",
    "# count number of trips where stop_time < start_time\n",
    "n_time_travel = (trip[\"stop_time\"] < trip[\"start_time\"]).sum()\n",
    "print(f\"The {n_time_travel} trips for which stop_time < start_time:\")\n",
    "# show the few outliers\n",
    "trip.query(\"stop_time < start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recompute the trip duration in seconds from the stop and start times:\n",
    "trip[\"computed_duration\"] = (trip[\"stop_time\"]-trip[\"start_time\"]).dt.seconds\n",
    "# and see if there is more than a 1min = 60s difference with trip_duration:\n",
    "trip[\"over_1min\"] = (trip[\"computed_duration\"]-trip[\"trip_duration\"]).abs() > 60\n",
    "# show the few outliers\n",
    "print(\"Trips with over 1min difference between stop_time-start_time and trip_duration\")\n",
    "# we focus on the stop_time > start_time trips, the stoptime < startime trips are shown just above\n",
    "trip.query(\"(stop_time > start_time) & over_1min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually very few errors (9 out of 236065). Let's filter out the bad rows and drop the utility columns\n",
    "`computed_duration` and `over_1min` we have created for the sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilde is the logical NOT operator\n",
    "trip = trip.query(\"(stop_time > start_time) & ~over_1min\")\n",
    "# dropping columns\n",
    "trip = trip.drop(columns=[\"computed_duration\", \"over_1min\"])\n",
    "print(f\"Filtered trip dataset: we now have {trip.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one final check. You may have notice that the trip dataset contains the ID of the stations where the trip originated and terminated. But do we recover every `station_id` of the `trip` dataset in the `station` dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = station[\"station_id\"].unique().tolist()\n",
    "from_station_ids = trip[\"from_station_id\"].unique().tolist()\n",
    "to_station_ids = trip[\"to_station_id\"].unique().tolist()\n",
    "trip_station_ids = set(from_station_ids + to_station_ids)\n",
    "not_in_station = [\n",
    "    station_id for station_id in trip_station_ids\n",
    "    if station_id not in station_ids\n",
    "]\n",
    "print(\n",
    "    f\"{len(not_in_station)} / {len(trip_station_ids)} ids not recovered in station\", \n",
    "    not_in_station\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These station ids indeed seem special, maybe they correspond to a repair / maintenance shop , or refer to \"lost\" bikes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This dataset is very clean : all fields have a clear meaning that match the data type, no weird values. Note that such high quality data is more the exception than the rule...\n",
    "\n",
    "Usually data is very messy, and you will spend a considerable amount of time cleaning it.\n",
    "Unfortunately people are very creative to mess things up: each dataset is messy in its own unique way.\n",
    "Fields can be incoherent (start time $>$ end time), using $-1$ or $999$ as missing values, aberrant values, duplicated rows, unintelligble column names, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Your turn ! Check that:\n",
    "\n",
    "- each trip corresponds to a single record in the `trip` dataset (no duplicates)\n",
    "- the values for `gender`, `user_type` and `birth_year` make sense\n",
    "- the number of bikes roughly agrees with the data description\n",
    "- the values in `weather` data make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_sanity_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic visualization\n",
    "\n",
    "For data visualization we will be using the [Altair](https://altair-viz.github.io/) package, a good place to start is the [gallery](https://altair-viz.github.io/gallery/index.html) where you will find many basic and advanced examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.data_transformers.enable('csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather\n",
    "\n",
    "Let's take at the look at the number of events (fog/snow/rain/sun) for each month. In a declarative visualization library like Altair, it's very simple: just declare *what* you would like to see, not *how* to do it. In our case we would like to show, for each \n",
    "$x$ = month and $y$ = type of events, the number of records (or count) in our dataset, \n",
    "and display this count by a circle of varying size for example.\n",
    "Well, that's pretty much all you need to write in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a chart using the `weather` dataset\n",
    "alt.Chart(weather).mark_circle().encode(\n",
    "    x=\"month(date)\", y='events', size=\"count()\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is a lot of sunny days during spring-summer, rainy days in autumn-winter. \n",
    "There are very few snowing days, and only during winter. It seems reasonable (Seattle has a warm-temperate climate) !\n",
    "\n",
    "Now let's take a look at the daily temperature: we want to show the mean temperature, but also the min-max temperature interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean temperature is represented as a line\n",
    "line  = alt.Chart(weather).mark_line().encode(\n",
    "    x='date', y='mean_temperature'\n",
    ") \n",
    "# the min-max temperature interval is represented by a shaded (opacity=0.2) area\n",
    "area = alt.Chart(weather).mark_area(opacity=0.2).encode(\n",
    "    x='date', \n",
    "    # we change the y-axis title and do not enforce the y-scale to start at zero\n",
    "    y=alt.Y('min_temperature', scale=alt.Scale(zero=False), title=\"temperature\"),\n",
    "    y2='max_temperature'\n",
    ")\n",
    "# we can zoom and pan along the x-axis\n",
    "(line + area).interactive(bind_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see a seasonal trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Explore the weather dataset ! The list of columns in the weather dataset is given below. Don't forget to take a look at the Altair [examples gallery](https://altair-viz.github.io/gallery/index.html), there is probably an example very close to the visualization you would like to do ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group-by   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Trips\n",
    "\n",
    "Let's now look at the daily number of trips, for each user type (member or short-term pass holder). To do this, we need to group by date and user type and simply count the number of records (method `.size()` in pandas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the date from start_time\n",
    "trip[\"date\"] = pd.to_datetime(trip[\"start_time\"].dt.date)\n",
    "# daily number of trips per user_type\n",
    "daily_trips = trip.groupby([\"date\",\"user_type\"]).size().rename(\"trips\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the computed time-series. The chart below is interactive: you can pan or zoom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(daily_trips).mark_line().encode(\n",
    "    x='date', y=\"trips\", color=\"user_type\"\n",
    ").interactive(bind_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have notice a very strong weekly pattern ! Interestingly this pattern is opposite for members and short-term pass holders.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : weekly trend\n",
    "\n",
    "Explain the weekly trend. Using an appropriate groupby aggregation and visualization, investigate if there is something like a \"commute\" versus \"leisure ride\" usage of the cycle share system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_user_type.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : most popular routes\n",
    "\n",
    "Find the most popular routes for members and short-term pass holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_routes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice : trips per bike\n",
    "\n",
    "Show that there is clear relationship between the number of trips done a on bike and the number of days \n",
    "the bike has been in service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_bikes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : trip durations\n",
    "\n",
    "In the pronto cycle share system only the first 30 minutes are free, afterwards one must pay an additional fee. Are members and short-time pass holders equally aware of the 30 minutes limit ?\n",
    "\n",
    "**Hint** : look at the `.value_counts()` method in pandas. How would you compute a normalized histogram on the trip durations in minutes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_durations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather influence\n",
    "\n",
    "We have computed the daily number of trips and we also have daily weather data. To enrich our daily trips with meteorological  data we simply need to join the `daily_trips` dataframe with the `weather` dataframe on the `date` column. The function `merge` in pandas is used to compute joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join daily_trips with weather\n",
    "daily_trips_joined = pd.merge(daily_trips, weather, on=\"date\", how=\"left\")\n",
    "daily_trips_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : temperature trend\n",
    "\n",
    "Does the temperature affect the number of trips ? Do you observe the same trend for members and short-term pass holders, during workdays and weekends ? Do the weather events (rain/sun/fog/snow) affect the number of trips ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_temperature.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : elevation trend\n",
    "\n",
    "Seattle is a quite bumpy city, do the elevations affect the trips ?\n",
    "The elevation for each station was fetched from the google maps elevation API, see the [cycle-share-dataset/Fetch_APIs.ipynb](cycle-share-dataset/Fetch_APIs.ipynb) notebook.\n",
    "\n",
    "**Hint** try to find the elevations for both `from_station_id` and `to_station_id` in the `routes` dataset. You can use for instance the `.rename(columns={...})` method and the `pd.merge()` function to do this join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevations = pd.read_csv(os.path.join(data_dir, \"elevations.csv\"))\n",
    "elevations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exo_elevation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps\n",
    "\n",
    "## Stations\n",
    "\n",
    "It's always fun and instructive to visualize geographical data on a map ! Let's display the stations on the Seattle map. If you zoom enough to see a station, and hover over the station to display its name, you should convince youself that the locations (latitude/longitude) are consistent with the station names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps, Marker, MarkerCluster, CircleMarker\n",
    "\n",
    "station_map = Map(center=(47.63, -122.3), zoom=12)\n",
    "markers = tuple(\n",
    "    Marker(location=(row[\"lat\"],row[\"long\"]), title=row[\"name\"], draggable=False)\n",
    "    for _, row in station.iterrows()\n",
    ")\n",
    "station_map.add_layer(MarkerCluster(markers = markers))\n",
    "station_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on the station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip[\"weekend\"] = trip[\"day_name\"].isin([\"Saturday\", \"Sunday\"])\n",
    "trip[\"member\"] = trip[\"user_type\"]==\"Member\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_infos = station.set_index(\"station_id\")\n",
    "station_infos = station_infos.join(\n",
    "    elevations.set_index(\"station_id\")[\"elevation\"]\n",
    ")\n",
    "station_infos = station_infos.join(\n",
    "    trip.groupby(\"from_station_id\")[[\"member\",\"weekend\"]].mean()\n",
    ")\n",
    "station_infos = station_infos.join(\n",
    "    trip.groupby(\"from_station_id\").size().rename(\"departures\")\n",
    ")\n",
    "station_infos = station_infos.join(\n",
    "    trip.groupby(\"to_station_id\").size().rename(\"arrivals\")\n",
    ")\n",
    "station_infos[\"outflow\"] = (\n",
    "    station_infos[\"departures\"] - station_infos[\"arrivals\"]\n",
    ").div(\n",
    "    station_infos[\"departures\"] + station_infos[\"arrivals\"]\n",
    ")\n",
    "station_infos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_infos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing stations features on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HTML, interact\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "def viridis(x):\n",
    "    cmap = matplotlib.cm.get_cmap(\"viridis\")\n",
    "    return matplotlib.colors.to_hex(cmap(x))\n",
    "\n",
    "\n",
    "station_map = Map(center=(47.63, -122.3), zoom=12)\n",
    "circles = {\n",
    "    station_id: CircleMarker(\n",
    "        location=(row[\"lat\"],row[\"long\"]),\n",
    "        fill_color=viridis(row[\"member\"]),\n",
    "        radius=7, fill_opacity=1, stroke=False\n",
    "    )\n",
    "    for station_id, row in station_infos.iterrows()\n",
    "}\n",
    "for station_id, circle in circles.items():\n",
    "    station_map.add_layer(circle)\n",
    "    row = station_infos.loc[station_id]\n",
    "    circle.popup = HTML(\n",
    "        f\"{station_id}<br><small>{row['name']}\"\n",
    "        \"</small>\"\n",
    "    )\n",
    "\n",
    "def min_max_scaling(x):\n",
    "    return (x-x.min()).div(x.max()-x.min())\n",
    "    \n",
    "    \n",
    "def refresh_circles(feature):\n",
    "    scaled_feature = min_max_scaling(station_infos[feature])\n",
    "    for station_id, circle in circles.items():\n",
    "        scaled_value = scaled_feature.loc[station_id]\n",
    "        circle.fill_color = viridis(scaled_value)\n",
    "\n",
    "interact(\n",
    "    refresh_circles, \n",
    "    feature=[\"member\", \"weekend\", \"departures\", \"install_dockcount\", \"outflow\", \"elevation\",\"lat\",\"long\"]\n",
    ")\n",
    "station_map"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "basic_stack",
   "language": "python",
   "name": "basic_stack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
