{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning for cats vs. dogs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import adam\n",
    "from keras import applications\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use labeled images of cats and dogs to train a classifier able to distinguish between them. This is a task which, like many others in computer vision, we have learned how to master in the few past years, mainly because of *convolutional neural networks*.\n",
    "\n",
    "There has been a [competition on Kaggle](https://www.kaggle.com/c/dogs-vs-cats) for this, back in 2014. Pierre Sermanet, a student of Yann LeCun, took 1st place, with an impressive 98.9% of accuracy on the test set. He briefly explains how he did it in this [Google+ post](https://plus.google.com/+PierreSermanet/posts/GxZHEH9ynoj):\n",
    "\n",
    "> I just won the Dogs vs. Cats Kaggle competition, using the deep learning library I wrote during my PhD: OverFeat http://cilvr.nyu.edu/doku.php?id=code:start\n",
    ">\n",
    "> My system was pre-trained on ImageNet (ILSVRC12 classification dataset) and subsequently refined on the cats and dogs data. I limited my number of submissions to 5 to avoid test set tuning and obtained 1st place against 215 other teams with 98.9% accuracy.\n",
    "\n",
    "But what exactly does it mean to *pre-train* a model? Well, let's figure out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the data from the Kaggle contest. The dataset is quite big: there are 25000 training samples, and 12500 test samples, all of different sizes. We are thus going to work with only a few of them: we are taking 3000 samples from the training set, and using 2000 of them for training and 1000 for validation.\n",
    "\n",
    "**BEFORE PROCEEDING**: please download the smaller dataset [here](https://filesender.renater.fr/?s=download&token=0b4ecc42-58ed-f056-ceed-468638a796a4) and extract inside the `lec4/mldata` folder.\n",
    "\n",
    "Let's load the data - Keras has some nice routines for that purpose. We start by loading a single image, just to see how it looks like (try changing the filename to see some others). Note we are doing color images, and thus working with 3-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and transform it to a Numpy array\n",
    "img = load_img(\"mldata/catsvsdogs/train/cats/cat.2.jpg\")\n",
    "x = img_to_array(img)\n",
    "print(\"array size:\", x.shape)\n",
    "\n",
    "# Show image\n",
    "plt.imshow(x / 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify some training parameters, and create a *generator* for the data, by using a quite convenient tool from Keras known as `ImageDataGenerator`. Essentially it iterates through the images in the directory, and preprocess them for us.\n",
    "\n",
    "Below, we are rescaling each of the images, so that each pixel intensity goes from 0 to 1 instead of 0 to 255; and also adding some noise to the image so that the classifier becomes more robust, a technique known as *data augmentation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "\n",
    "n_train_samples = 2000\n",
    "n_valid_samples = 1000\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# Set up generator for training and validation images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_generator = train_datagen.flow_from_directory(\"mldata/catsvsdogs/train\",\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"binary\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_generator = test_datagen.flow_from_directory(\"mldata/catsvsdogs/validation\",\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will write down our neural network using Keras and train it. Below we use a neural network with 3 convolutional layers, which are supposed to learn relevant features, and 2 more dense layers, that will use the learned features to classify the image as a cat or a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify network architecture: 3 conv. layers w/ ReLU activations + 2 dense layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit our model\n",
    "#model.fit_generator(\n",
    "    #train_generator,\n",
    "    #steps_per_epoch=n_train_samples // batch_size,\n",
    "    #epochs=epochs,\n",
    "    #validation_data=valid_generator,\n",
    "    #validation_steps=n_valid_samples // batch_size)\n",
    "\n",
    "#model.save_weights(\"mldata/cnn_catsvsdogs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (or if you don't want to wait you can't just load the weights below)\n",
    "model.load_weights(\"mldata/cnn_catsvsdogs.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, that took a long time (unless you have a GPU!). If only we could use weights that we already trained on other datasets...\n",
    "\n",
    "Let's see what's the accuracy we get on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate_generator(valid_generator, steps=n_valid_samples//batch_size)\n",
    "print(\"accuracy on test set:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad -- with such accuracy we would be among the top half in the Kaggle contest (see ref. 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our CNN took a very long time. What if we replace it by another one, which was trained in a *similar dataset*? After all, the important features should be more or less the same, right?\n",
    "\n",
    "People have been using lots of computational resources, training very deep networks over huge datasets. Luckily they have made their weights available, so we can use them! Below we gonna use a 16-layer convolutional neural network that was trained over the Imagenet dataset, known as [VGG-16](http://www.robots.ox.ac.uk/%7Evgg/research/very_deep/).\n",
    "\n",
    "![VGG16](vgg16.png)\n",
    "\n",
    "Let's do it in steps: first we create generators as before (now without any data augmentation) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "n_train_samples = 160\n",
    "n_valid_samples = 80\n",
    "\n",
    "# Set up generator for training and validation images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\"mldata/catsvsdogs_small/train\",\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=None,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_generator = test_datagen.flow_from_directory(\"mldata/catsvsdogs_small/validation\",\n",
    "                                                   target_size=(img_width, img_height),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=None,\n",
    "                                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then we load the VGG16 network, and run our samples through it (this might take a while...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 weights and generate features for each image\n",
    "vgg16 = applications.VGG16(include_top=False, weights=\"imagenet\")\n",
    "train_features = vgg16.predict_generator(train_generator, n_train_samples // batch_size)\n",
    "valid_features = vgg16.predict_generator(valid_generator, n_valid_samples // batch_size)\n",
    "\n",
    "# Create array containing labels\n",
    "train_labels = np.hstack((np.zeros(n_train_samples // 2), np.ones(n_train_samples // 2)))\n",
    "valid_labels = np.hstack((np.zeros(n_valid_samples // 2), np.ones(n_valid_samples // 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand what the VGG16 network is doing. First let's see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features[0, :].shape)\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(7, 7))\n",
    "for i in range(64):\n",
    "    axs[i // 8, i % 8].imshow(train_features[0, :, :, i])\n",
    "    axs[i // 8, i % 8].get_xaxis().set_visible(False)\n",
    "    axs[i // 8, i % 8].get_yaxis().set_visible(False)\n",
    "fig.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of 512 images of size 4x4, but looking at them doesn't reveal us much... Let us look at the output of the layers one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that reads image and returns each layer output\n",
    "input_img = vgg16.input\n",
    "outputs = [layer.output for layer in vgg16.layers]\n",
    "functors = [K.function([input_img], [out]) for out in outputs]\n",
    "\n",
    "# Input first image in the training set to this function\n",
    "train_generator.reset()\n",
    "batch = train_generator.next()\n",
    "\n",
    "img = batch[0][np.newaxis, :]\n",
    "layer_outputs = [func([img]) for func in functors]\n",
    "\n",
    "for i in range(len(layer_outputs)):\n",
    "    print(\"layer %d shape: %s\" % (i, layer_outputs[i][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of `layer` below, from 1 to 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(14, 14))\n",
    "for i in range(64):\n",
    "    axs[i // 8, i % 8].imshow(layer_outputs[layer][0][0, :, :, i], cmap=\"gray\")\n",
    "    axs[i // 8, i % 8].get_xaxis().set_visible(False)\n",
    "    axs[i // 8, i % 8].get_yaxis().set_visible(False)\n",
    "fig.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand each layer of a CNN as performing multiple image processing tasks in parallel - detecting edges, sharpening, blurring - [by doing convolutions with the learned filters](https://en.wikipedia.org/wiki/Kernel_(image_processing). These tasks are performed not on the original image, but on the output of the previous layer; that's why the output of the rightmost layers become very hard to interpret.\n",
    "\n",
    "Which task is performed depends on which filter is used. Here we are using the filters embedded on VGG16, but in principle we could learn them, if we were to train a CNN from scratch.\n",
    "\n",
    "An interesting exercise, which provides a way of interpreting the output of the rightmost layers, is to find the image [which maximizes the activation of a certain filter](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html).\n",
    "\n",
    "Finally, we take the output of the VGG16 network and plug it into the 2-layer classifier we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our architecture (quite simple now!)\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_features.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model and print summary\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=adam(1e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit(train_features, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(valid_features, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(valid_features, valid_labels)\n",
    "print(\"accuracy on test set:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good no? And definitely much faster :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 8 images at random, pass them through VGG16 and then through classifier\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "generator = datagen.flow_from_directory(\"mldata/catsvsdogs/validation\",\n",
    "                         target_size=(img_width, img_height),\n",
    "                         batch_size=8,\n",
    "                         class_mode=\"binary\")\n",
    "batch = generator.next()\n",
    "features = vgg16.predict(batch[0])\n",
    "probs = model.predict_proba(features)\n",
    "\n",
    "# Show images together with probabilities\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    axs[i // 4, i % 4].imshow(batch[0][i])\n",
    "    axs[i // 4, i % 4].set_title(\"prob. dog: %.2f\" % (probs[i]))\n",
    "    \n",
    "    axs[i // 4, i % 4].get_xaxis().set_visible(False)\n",
    "    axs[i // 4, i % 4].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "2. https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "3. https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n",
    "4. https://github.com/abursuc/dldiy-practicals/blob/master/10_05_lesson1.ipynb\n",
    "5. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/dogs_cats_redux.ipynb\n",
    "6. http://www.cs.toronto.edu/~frossard/post/vgg16/\n",
    "7. https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
