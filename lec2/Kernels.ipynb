{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go back to regression problems, and to make things more exciting, we will work with real-world data! We are going to load the Boston dataset of house prices, and try to predict the value of a house based on its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dset = datasets.load_boston()\n",
    "print(dset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we randomly split the data in a training and test sets. We also apply some pre-processing routines, something fundamental when working with real data. Here we *standardize* each feature (i.e. make them zero-mean, unit-variance), and remove the mean of the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = dset.data, dset.target\n",
    "print(\"number of samples/features: %d, %d\" % X.shape)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "print(\"training/test samples: %d, %d\" % (len(y_train), len(y_test)))\n",
    "\n",
    "# Center and scale features and observations\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "ymean = np.mean(y_train)\n",
    "y_train = y_train - ymean\n",
    "y_test = y_test - ymean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating kernels via random feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next aim is to map the problem to a dimension $D>P$ (ideally, $D\\gg P$). Our aim is to embed the original feature space in a higher dimensional space, in the hope that the problem can be treated by linear methods in the larger space. Then, linear maps are not very useful, because they would preserve the dimensionality of the original problem. So, we also have to use non-linear maps.\n",
    "\n",
    "We thus define $z_i = f(x_i)$ using a non-linear map $f: \\mathbb{R}^P \\to \\mathbb{R}^D$ with $D > P$. It was shown in the lectures that when $D\\to\\infty$, the scalar products $z_i^T z_j$ converge to a **kernel function** $K({x}_i, {x}_j)$.\n",
    "\n",
    "For many interesting learning procedures, the embedding in higher dimension is therefore equivalent to the replacement of scalar products by a kernel. This essentially take us to infinite dimension!\n",
    "\n",
    "Usually, kernels are normalized in such a way that $K(x,x)=1$.\n",
    "In the following we focus more specifically on the RBF kernel $K({x}_i, {x}_j) = e^{-\\frac{1}{2} \\| {x}_i - {x}_j \\|^2_2}$. \n",
    "\n",
    "**Exercise**: compute and plot the kernelized Gram matrix $K_{ij} = K(x_i,x_j)$ for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute kernelized Gram matrix using the original kernel ...\n",
    "from scipy.spatial.distance import cdist\n",
    "gram_orig =\n",
    "\n",
    "# ... and plot it.\n",
    "plt.imshow(gram_orig, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rks2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, it's often more efficient to *approximate* the kernel by expanding it in a given basis (usually Fourier) and then approximately performing the integral via importance sampling. This is equivalent to consider a *random* map (as we did in the dimensional reduction case, but now for a non-linear map)\n",
    "\n",
    "$$\\phi({x}_i, {w}) = e^{i w^T x_i}$$\n",
    "\n",
    "with $w$ extracted i.i.d. from some $p(w)$ and \n",
    "\n",
    "$$z_i = f(x_i) = \\frac1{\\sqrt{D}}\\{ \\phi({x}_i, w_1),\\cdots, \\phi({x}_i, {w}_D) \\}\n",
    " =\\frac1{\\sqrt{D}}\\{ e^{i w_1^T x_i},\\cdots, e^{i w_D^T x_i} \\} \\in \\mathbb{C}^D\n",
    "$$\n",
    "\n",
    "Note that for convenience we used $\\mathbb{C}^D$ as a feature space, which has dimension $2D$ instead of $D$.\n",
    "Then\n",
    "\n",
    "$$K({x}_i, {x}_j) = z_i^T z_j \n",
    "\\approx \\frac{1}{D} \\sum_{a = 1}^D \\phi({x}_i, {w}_a) \\phi^*({x}_j, {w}_a)\n",
    "\\approx \\int dw p(w) e^{i w^T (x_i - x_j)}\n",
    "$$\n",
    "\n",
    "If we choose $p(w) = e^{-\\frac12 ||w||_2^2}/(2 \\pi)^{P/2}$ as a Gaussian with unit variance, then we get the desired kernel.\n",
    "This is the so-called *random kitchen sinks* algorithm. Let's implement it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RKS](rks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_features(X, n_projections):\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Sample w\n",
    "    w =\n",
    "    \n",
    "    # Compute z\n",
    "    z =\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rks3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the approximated Gram matrix approaches the exact one as `n_projections` increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Gram matrix using the approximate kernel\n",
    "X_transf = generate_nonlinear_features(X_train, 500)\n",
    "gram_approx = X_transf.dot(X_transf.T)\n",
    "\n",
    "# Plot Gram matrices using both exact and approximate kernels\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 6))\n",
    "p0 = axs[0].imshow(gram_orig, interpolation=\"nearest\", vmin=0, vmax=0.1)\n",
    "p1 = axs[1].imshow(gram_approx, interpolation=\"nearest\", vmin=0, vmax=0.1)\n",
    "plt.colorbar(p0, ax=axs[0], shrink=0.75)\n",
    "plt.colorbar(p1, ax=axs[1], shrink=0.75)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of exact and approximate kernels\n",
    "plt.plot(gram_orig.ravel(), gram_approx.ravel(), \"o\")\n",
    "plt.plot([0, 1], [0, 1])\n",
    "\n",
    "plt.xlabel(\"original kernel\")\n",
    "plt.ylabel(\"approximated kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many projections do we need before $\\langle \\phi (x) \\phi(\\mu)\\rangle$ gives a good approximation to $K(x, \\mu) = e^{-\\frac{(x - \\mu)^2}{2}}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1.3\n",
    "\n",
    "# Generate 1D grid\n",
    "xx = np.linspace(-5, 5, 101)\n",
    "\n",
    "# Compute kernel approximation\n",
    "def compute_approx(grid, n_projections):\n",
    "    phi = generate_nonlinear_features((grid - mu).reshape(-1, 1), n_projections)\n",
    "    return np.sum(phi, 1) / np.sqrt(n_projections)\n",
    "\n",
    "# Plot approximation and exact kernels\n",
    "plt.plot(xx, compute_approx(xx, 100))\n",
    "plt.plot(xx, np.exp(-.5 * (xx - mu) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our kernel approximation performs in the solution of linear models. We first start with doing ridge regression while replacing the Gram matrix by the (exact) kernel.\n",
    "\n",
    "This amounts to map $x_i$ into $z_i$, and then doing a ridge regression $y = Z w$ in the new feature space. The solution is \n",
    "\n",
    "$$w = Z^T (Z Z^T + \\lambda I_N)^{-1} y = Z^T (K + \\lambda I_N)^{-1} y$$\n",
    "\n",
    "and if we are shown new data (e.g. the test set) we first compute $Z_{test}$ and then\n",
    "\n",
    "$$y = Z_{test} w = Z_{test} Z^T (K + \\lambda I_N)^{-1} y = K^{test}(K + \\lambda I_N)^{-1} y$$\n",
    "\n",
    "Therefore, we only need to compute the kernel $K_{ij} = K(x_i,x_j)$ and the test kernel $K^{test}_{ij} = K(x^{test}_i,x_j)$ to perform the ridge regression, without need to really compute the features $z_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel_ridge(X, y, lamb=0.05):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Obtain kernelized Gram matrix and perform least-squares estimation\n",
    "    corr =\n",
    "    coef =\n",
    "    \n",
    "    # Compute training error\n",
    "    error = np.mean((y - corr.T.dot(coef)) ** 2)\n",
    "    print(\"error on training set: %g\" % error)\n",
    "    \n",
    "    return coef\n",
    "\n",
    "def test_kernel_ridge(X_test, y_test, X_train, coef):\n",
    "    corr =\n",
    "    \n",
    "    # Compute test (generalization) error\n",
    "    error = np.mean((y_test - corr.T.dot(coef)) ** 2)\n",
    "    print(\"error on test set: %g\" % error)\n",
    "    \n",
    "    return error\n",
    "\n",
    "coef = train_kernel_ridge(X_train, y_train)\n",
    "kernel_ridge_error = test_kernel_ridge(X_test, y_test, X_train, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rks4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a sanity check here, just to see if these numbers make sense. What happens if we try to do linear ridge regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge(X, y, lamb=0.05):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    coef = np.linalg.lstsq(X.T.dot(X) + lamb * np.eye(n_features), X.T.dot(y))[0]\n",
    "    error = np.mean((y_train - X_train.dot(coef)) ** 2)\n",
    "    print(\"error on training set w/ ridge: %g\" % error)\n",
    "        \n",
    "    return coef\n",
    "    \n",
    "coef_ridge = train_ridge(X_train, y_train)\n",
    "ridge_test = np.mean((y_test - X_test.dot(coef_ridge)) ** 2)\n",
    "print(\"error on test set w/ ridge: %g\" % (ridge_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we observe is that the kernel ridge regression is badly overfitting the train dataset, but yet it performs a bit better than the linear ridge regression on the test dataset.\n",
    "\n",
    "Let us also check our results with the scikitlearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import kernel_ridge\n",
    "\n",
    "reg = kernel_ridge.KernelRidge(alpha=0.05, kernel=\"rbf\", gamma=0.5)\n",
    "reg.fit(X_train, y_train)\n",
    "sklearn_train = np.mean((y_train - reg.predict(X_train)) ** 2)\n",
    "sklearn_test = np.mean((y_test - reg.predict(X_test)) ** 2)\n",
    "print(\"error on training/test set w/ sklearn: %g/%g\" % (sklearn_train, sklearn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results seem to match those of scikitlearn. To conclude, we can quickly check the behavior as a function of the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty arrays\n",
    "lam_dom = np.logspace(-2, 2, 100)\n",
    "\n",
    "train_err_k = np.zeros(len(lam_dom))\n",
    "test_err_k = np.zeros(len(lam_dom))\n",
    "train_err_nok = np.zeros(len(lam_dom))\n",
    "test_err_nok = np.zeros(len(lam_dom))\n",
    "\n",
    "n_samples, n_features = X_train.shape\n",
    "train_corr = np.exp(-.5 * cdist(X_train, X_train) ** 2)\n",
    "test_corr = np.exp(-.5 * cdist(X_train, X_test) ** 2)\n",
    "\n",
    "# Compute different metrics for many $\\lambda$\n",
    "for (i, lamb) in enumerate(lam_dom):\n",
    "    coef = np.linalg.lstsq(train_corr + lamb * np.eye(n_samples), y_train)[0]\n",
    "    train_err_k[i] = np.mean((y_train - train_corr.T.dot(coef)) ** 2)\n",
    "    test_err_k[i] = np.mean((y_test - test_corr.T.dot(coef)) ** 2)\n",
    "    \n",
    "    coef = np.linalg.lstsq(X_train.T.dot(X_train) + lamb * np.eye(n_features), X_train.T.dot(y_train))[0]\n",
    "    train_err_nok[i] = np.mean((y_train - X_train.dot(coef)) ** 2)\n",
    "    test_err_nok[i] = np.mean((y_test - X_test.dot(coef)) ** 2)\n",
    "    \n",
    "# Visualize\n",
    "plt.semilogx(lam_dom,train_err_k, label='Kernel, train')\n",
    "plt.semilogx(lam_dom,test_err_k, label='Kernel, test')\n",
    "plt.semilogx(lam_dom,train_err_nok, label='No kernel, train')\n",
    "plt.semilogx(lam_dom,test_err_nok, label='No kernel, test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace the exact kernel with the approximate one. We first rewrite the `generate_nonlinear_features` function so as to receive/return $\\omega$ values; this is necessary in order to compute $K$ for new points outside the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_features(X, n_projections, w = None):\n",
    "    n_features = X.shape[1]\n",
    "    if w is None:\n",
    "        w = np.random.randn(n_features, n_projections)\n",
    "    z = np.hstack((np.cos(X.dot(w)), np.sin(X.dot(w)))) / np.sqrt(n_projections)\n",
    "    return z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to rewrite the functions above using approximate kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rks(X, y, n_projections, lamb=0.05):\n",
    "    n_samples, n_features = X.shape\n",
    "    # Obtain random features and perform least-squares estimation\n",
    "    z, w = generate_nonlinear_features(X, n_projections)\n",
    "    coef = z.T.dot(np.linalg.lstsq(z.dot(z.T) + lamb * np.eye(n_samples), y)[0])\n",
    "    # Compute training error\n",
    "    train_error = np.mean((y - z.dot(coef)) ** 2)\n",
    "    return coef, w, train_error\n",
    "\n",
    "def test_rks(X, y, coef, w):\n",
    "    z, _ = generate_nonlinear_features(X, w.shape[1], w)\n",
    "    # Compute test (generalization) error\n",
    "    test_error = np.mean((y_test - z.dot(coef)) ** 2)\n",
    "    return test_error\n",
    "\n",
    "coef, w, train_error = train_rks(X_train, y_train, 500)\n",
    "test_error = test_rks(X_test, y_test, coef, w)\n",
    "print(\"error on train set: %g\" % train_error)\n",
    "print(\"error on test set: %g\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the performance as a function of `n_projections`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test error for many values of n_projections\n",
    "ks = np.arange(200, 5000, 200)\n",
    "errors = np.zeros(len(ks))\n",
    "for (i, k) in enumerate(ks):\n",
    "    coef, w, _ = train_rks(X_train, y_train, k)\n",
    "    errors[i] = test_rks(X_test, y_test, coef, w)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(ks, errors)\n",
    "plt.axhline(kernel_ridge_error, c=\"k\", ls=\"-.\", lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Boston dataset example we had $N\\approx 500$ and $P=13$.\n",
    "We found that with around $D\\approx 1000$ projections, we can obtain roughly the same results that we obtained with the original kernel. Do we gain something using the projections?\n",
    "\n",
    "To understand this, let us check the computational time for the different options:\n",
    "\n",
    "$\\bullet$ If we use the original kernel ridge regression, the kernel has size $N\\times N$ and computing each element requires $P$ operations, therefore its computation takes time $\\approx N^2 P$ and its inversion takes time $\\approx N^3$. The total time is $N^2 \\max(P, N)$.\n",
    "\n",
    "$\\bullet$ If we use $D$ random projections, then we need time $P D$ to construct the $w$, and time $N P D$ to compute the matrix $Z$. But then we have two options. If we use the formula\n",
    "\n",
    "$$w_1 = Z^T (Z Z^T + \\lambda I_N)^{-1} y$$\n",
    "\n",
    "then we need to construct the matrix $Z Z^T$, which takes time $N^2 D$, and invert it, which takes time $N^3$. The total time is then $\\max(N P D, N^2 D, N^3)$.\n",
    "**But**, we can also use the equivalent formula\n",
    "\n",
    "$$w_2 = (Z^T Z + \\lambda I_D)^{-1} Z^T y$$\n",
    "\n",
    "in which case we need to construct the matrix $Z^T Z$, which takes time $D^2 N$, and invert it, which takes time $D^3$. The total time is then $\\max(N P D, D^2 N, D^3)$.\n",
    "\n",
    "\n",
    "\n",
    "In the example above, we have small $P$, and we need $D>N$, hence the Kernel method takes time $\\approx N^3$, while for the random projections method it is better to use $w_1$, which takes time $N^2 D$, so in the end the kernel method is faster. However, in a \"big data\" setting where $N \\gg D \\gg P$, then the kernel method would take time $N^3$, while the random projection method using $w_2$ would take time $N D^2$, providing a huge speedup.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We didn't gain so much in using RKS in the previous section, partially because the dataset we were using was simple and small, as we discussed. Let us try it again with a bigger, more complex dataset.\n",
    "\n",
    "Moreover, we will use Support Vector Machines. Recall that a SVM is a classifier, defined as follows. We first find the normal vector\n",
    "\n",
    "$$\\hat{w} = \\text{argmin}_w  \\frac1N \\sum_{i=1}^N H[y_i (w \\cdot x_i - b)] + \\lambda ||w||^2  \n",
    "\\qquad \\qquad H(z) = \\max(0,1-z)$$\n",
    "\n",
    "Then, the classifier for a new data $x$ is\n",
    "\n",
    "$$y = \\text{sgn}[ \\hat{w} \\cdot x - b ]$$\n",
    "\n",
    "A kernel SVM consists, as before, in moving to a different feature space $z_i = f(x_i)$ and applying a linear SVM in the new feature space. Because it can be shown that $\\hat w = \\sum_{i=1}^N c_i y_i z_i$, one can express everything in terms of the kernel $K(x_i, x_j) = z_i \\cdot z_j$. \n",
    "\n",
    "The training of a kernel SVM requires us to evaluate the kernel multiple times and not only once as in ridge regression. That makes even more important to optimize the computation.\n",
    "\n",
    "We start by loading the MNIST dataset. We extract from it $N=5000$ samples (each with $P=784$ features), selected among the zeros and ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata(\"MNIST original\", data_home=\"../lec1\")\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Preprocess\n",
    "X = X / 255.\n",
    "\n",
    "mask = np.logical_or.reduce([(y == k) for k in [0, 1]])\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "n_samples = 5000\n",
    "samples = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "X, y = X[samples, :], y[samples]\n",
    "\n",
    "# Do train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "print(\"samples/features in training set: %d, %d\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the kernel SVM function from scikit-learn. The fit time complexity is more than quadratic with the number of samples $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import svm\n",
    "\n",
    "# Create kernel SVM, k(x) = exp(-gamma |x|^2)\n",
    "kernel_svm = svm.SVC(gamma=.2)\n",
    "\n",
    "# Fit kernel SVM\n",
    "kernel_svm_time = time()\n",
    "kernel_svm.fit(X_train, y_train)\n",
    "kernel_svm_error = np.mean(y_test != kernel_svm.predict(X_test))\n",
    "kernel_svm_time = time() - kernel_svm_time\n",
    "\n",
    "print(\"error on training set: %g\" % np.mean(y_train != kernel_svm.predict(X_train)))\n",
    "print(\"error on test set: %g\" % kernel_svm_error)\n",
    "print(\"time: %g\" % kernel_svm_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel SVM provides a perfect fit of the training dataset, with a good error on the test dataset.\n",
    "\n",
    "Finally, we repeat the process for the Fourier-approximated kernel SVM. We use the RBFSampler routine of sklearn, that approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform, via Random Kitchen Sinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "# Create Fourier-approximated SVM, k(x) = exp(-gamma |x|^2)\n",
    "map_fourier = RBFSampler(gamma=.2)\n",
    "fourier_svm = pipeline.Pipeline([(\"feature_map\", map_fourier),\n",
    "                                 (\"svm\", svm.LinearSVC())])\n",
    "\n",
    "# Fit Fourier-approximated SVM\n",
    "fourier_times = []\n",
    "fourier_train_errors = []\n",
    "fourier_test_errors = []\n",
    "\n",
    "sample_sizes = [100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "for k in sample_sizes:\n",
    "    fourier_svm.set_params(feature_map__n_components=k)\n",
    "    start = time()\n",
    "    fourier_svm.fit(X_train, y_train)\n",
    "    fourier_times.append(time() - start)\n",
    "    fourier_train_error = np.mean(y_train != fourier_svm.predict(X_train))\n",
    "    fourier_train_errors.append(fourier_train_error)\n",
    "    fourier_test_error = np.mean(y_test != fourier_svm.predict(X_test))\n",
    "    fourier_test_errors.append(fourier_test_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the results. We find that the fit time complexity is now linear in $D$, and we obtain an important speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].plot(sample_sizes, fourier_train_errors, label=\"Train\")\n",
    "axs[0].plot(sample_sizes, fourier_test_errors, label=\"Test\")\n",
    "axs[0].axhline(kernel_svm_error, c=\"k\", ls=\"-.\", lw=2)\n",
    "axs[0].set(ylabel=\"error\", xlabel=\"D\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(sample_sizes, fourier_times)\n",
    "axs[1].axhline(kernel_svm_time, c=\"k\", ls=\"-.\", lw=2)\n",
    "axs[1].set(ylabel=\"runtime\", xlabel=\"D\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
