{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Features and Kernels\n",
    "\n",
    "In this lecture, we are going to discuss how changing the feature space can lead to computational advantages, or improved performances.\n",
    "\n",
    "Suppose we have $N$ samples $\\{x_i\\}_{i=1\\cdots N}$, each containing $P$ features. We arrange our samples into a matrix $X$ of size $N\\times P$, such that each row is a sample:\n",
    "\n",
    "$$X = \\left(\\begin{matrix} x_1  \\\\ x_2 \\\\ \\cdots \\\\ x_N\n",
    "\\end{matrix}\\right)$$\n",
    "\n",
    "We now want to introduce a **feature map**, i.e. to construct a function $z_i = f(x_i) \\in R^D$ that maps the points to a new feature space of different dimension. We also define a matrix $Z$ of size $N \\times D$, as before:\n",
    "\n",
    "$$Z = \\left(\\begin{matrix} z_1  \\\\ z_2 \\\\ \\cdots \\\\ z_N\n",
    "\\end{matrix}\\right) = f(X)$$\n",
    "\n",
    "We are going to discuss two different applications:\n",
    "\n",
    "$\\bullet$ In the first case, we consider $D$ small, i.e. we want to reduce the dimension of the feature space, while preserving the distances between points, i.e. the Gram matrix $X X^T \\sim Z Z^T$. This is useful to reduce the computational time needed to estimate the Gram matrix, in applications such as k-Nearest Neighbor classification\n",
    "\n",
    "$\\bullet$ In the second case, we consider $D$ large, i.e. we embed our points in a feature space of higher dimension. This is useful to improve e.g. classification performances, and in this case we want to choose the feature map in such a way that the Gram matrix is $Z Z^T \\sim K(x_i, x_j)$ for a given **kernel** $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random linear features - dimensional reduction\n",
    "## Johnson-Lindenstrauss lemma and dimensional reduction.  Application to k-Nearest Neighbor classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly speaking, the JL lemma guarantees that some maps preserve distance or inner products in an approximate way. More precisely\n",
    "\n",
    "> Given $0 < \\varepsilon < 1$, a set $\\{x_i\\}_{i=1\\cdots N}$ of $N$ points in $\\mathbb{R}^P$, and a number $D > 8 \\log(N) / \\varepsilon^2$, there is a linear map $f: \\mathbb{R}^P \\to \\mathbb{R}^D$ such that\n",
    ">\n",
    ">$$(1 - \\varepsilon) \\|x_i - x_j\\|^2 \\leq \\|f(x_i) - f(x_j)\\|^2 \\leq (1 + \\varepsilon) \\|x_i - x_j\\|^2$$\n",
    ">\n",
    "> for all $i, j \\in \\{1,\\cdots,N\\}$.\n",
    ">\n",
    "> Similarly, if $|x_i|^2<1$, $\\forall i$, then there is a linear map such that\n",
    ">\n",
    ">$$ |x_i \\cdot x_j - f(x_i) \\cdot f(x_j) |^2 < \\varepsilon $$\n",
    "\n",
    "Moreover, we actually know how to construct such maps; for instance, we know that a random $D \\times P$ matrix $A$ such that \n",
    "\n",
    "$$z = f(x) = \\frac{1}{\\sqrt{D}} A x,  \\qquad A_{ij} \\sim \\mathcal{N} (0, 1)$$\n",
    "\n",
    "satisfies the condition in the lemma with finite probability.\n",
    "See e.g. http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf for a simple proof.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JL lemma can be used to speedup the calculation of the Gram matrix $X X^T$, which in the original feature space takes $O(N^2 P)$ operations.\n",
    "The time needed to compute the random projections $z_i = A x_i/\\sqrt{D}$ is $O(N D P)$, and then $O(N^2 D)$ to compute $Z Z^T\\sim X X^T$. We thus obtain a speedup if \n",
    "\n",
    "$$N D \\max(P, N) \\ll N^2 P$$ \n",
    "\n",
    "According to the JL lemma we can choose $D\\sim \\log(N)$, and the speedup is of the order of \n",
    "\n",
    "$$D \\max\\left(\\frac1P, \\frac1N\\right) \\sim \\frac{ 8\\log N}{\\varepsilon^2} \\times \\max\\left(\\frac1P, \\frac1N\\right)$$.\n",
    "\n",
    "We are now going to apply this method to the k-Nearest Neighbor classification that we studied in lecture 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits dataset\n",
    "\n",
    "We are going to use the UCI ML hand-written digits datasets. This dataset contains $N$=1797 images, each being a set of $P = 8\\times 8=64$ pixels, each pixel value being an integer between 0 and 16. The image represents handwritten digits, from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(return_X_y=True)\n",
    "n_samples, n_features = np.shape(X)\n",
    "print(\"samples/features in data set: %d, %d\" % X.shape)\n",
    "\n",
    "# Do train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"samples/features in training set: %d, %d\" % X_train.shape)\n",
    "\n",
    "# Show an image\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(X_train[0, :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")\n",
    "\n",
    "# Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Show an image after scaling\n",
    "print(\"image label: %d\" % y_train[0])\n",
    "axs[1].imshow(X_train[0, :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional reduction\n",
    "\n",
    "Let's try the map mentioned above. We start with $N$ training samples, each containing $P$ features. We will generate $D$ new features by performing linear combinations of the features at random, with coefficients sampled from a standard Normal distribution. Our new set of samples is then gonna contain $N$ samples, each of dimension $D$ (instead of $P$).\n",
    "\n",
    "By playing a bit with the number of features in the transformed space, $D$, we can see how the Gramiam evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new set of samples by mapping from P features to D\n",
    "n_projections = 40\n",
    "A = np.random.randn(n_features, n_projections) / np.sqrt(n_projections)\n",
    "Z_train = X_train.dot(A)\n",
    "print(\"samples/features in transformed set: %d, %d\" % Z_train.shape)\n",
    "\n",
    "# Plot Gram matrix ...\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 6))\n",
    "\n",
    "# ... for the original set of samples ...\n",
    "p0 = axs[0].imshow(X_train.dot(X_train.T), interpolation=\"nearest\", vmin = -50, vmax = 50)\n",
    "plt.colorbar(p0, ax=axs[0], shrink = 0.75)\n",
    "\n",
    "# ... and for the transformed one.\n",
    "p1 = axs[1].imshow(Z_train.dot(Z_train.T), interpolation=\"nearest\", vmin = -50, vmax = 50)\n",
    "plt.colorbar(p1, ax=axs[1], shrink = 0.75)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to see how the distance between points change as we change the number of features in the transformed space.\n",
    "\n",
    "**Exercise**: make a scatter plot of distances between samples in original and transformed space, for $D = 10$ and $D = 40$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Generate Z for a given value of D\n",
    "def generate_random_features(X_train, n_projections):\n",
    "\n",
    "# Compute distance matrix for both original and transformed sets of features\n",
    "dists_orig =\n",
    "dists_proj =\n",
    "\n",
    "# Scatter plot of distances between samples in original/transformed spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rks1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform k-Nearest Neighbors classification in the original space and in the transformed space, and compare the two results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# Run the experiment for a range of values of k\n",
    "ks = np.r_[np.arange(1, 10), np.arange(10, 150, 30)]\n",
    "train_error_X = []\n",
    "test_error_X = []\n",
    "for k in ks:\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_error_X.append(1. - clf.score(X_train, y_train))\n",
    "    test_error_X.append(1. - clf.score(X_test, y_test))\n",
    "    print(\"k = %d; train error = %g, test error = %g\" % (k, train_error_X[-1], test_error_X[-1]))\n",
    "\n",
    "# Repeat the experiment using Z instead of X\n",
    "Z_test = X_test.dot(A)\n",
    "train_error_Z = []\n",
    "test_error_Z = []\n",
    "for k in ks:\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    clf.fit(Z_train, y_train)\n",
    "    train_error_Z.append(1. - clf.score(Z_train, y_train))\n",
    "    test_error_Z.append(1. - clf.score(Z_test, y_test))\n",
    "    print(\"k = %d; train error = %g, test error = %g\" % (k, train_error_Z[-1], test_error_Z[-1]))\n",
    "\n",
    "# Plot error as a function of the degrees of freedom\n",
    "plt.plot(len(y_train) / np.array(ks), train_error_X, \"-o\", label = \"X_train\")\n",
    "plt.plot(len(y_train) / np.array(ks), test_error_X, \"-o\", label = \"X_test\")\n",
    "plt.plot(len(y_train) / np.array(ks), train_error_Z, \"-o\", label = \"Z_train\")\n",
    "plt.plot(len(y_train) / np.array(ks), test_error_Z, \"-o\", label = \"Z_test\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"degrees of freedom $N / k$\")\n",
    "plt.ylabel(\"misclassification error\")\n",
    "plt.ylim((0., 0.15))\n",
    "\n",
    "print(\"minimum test error orig./transf.: %g/%g\" % (min(test_error_X), min(test_error_Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much did we gain/lost in terms of speed and accuracy when going from $P = 64$ to $D = 40$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
