This is a basic course in machine learning and statistical inference, with an emphasis on simple methods, theoretical understanding and practical exercises. The course will combine (and alternate) between methodology with theoretical foundations and practical computational aspects with exercices in python with scikit-learn and pytorch. The topics will be chosen from the following basic outline:

* Statistical theory : Maximum likelihood, Bayes, VC Bounds and Uniform convergence
* Supervised learning : Linear Regression, Ridge, Lasso, high Dimensional Data, Kernel methods, Boosting 
* Deep learning: multi-layer net, conv-net, auto-encoder
* Unsupervised learning : Mixture Models, PCA & Kernel PCA

### Computer requirement

All exercice will be in python. For instalation, we recomeand
(especially for mac) Anaconda (http://anaconda.org), more precisly
Python 3.7, with the following modules: numpy, scipy, matplotlib,
pandas, h5py, scikit-learn, scikit-image. All these can be installed
with anaconda.

For deep-learning, we shall use keras, tensorflow and pytorch.
All the exercices will be given as jupyter notebook, so these should be instaled as well.

### Lectures topic

<p> Lecture 1: Introduction to supervised machine learning. KNN, linear models, optimization. Some words on VC and rademacher  bounds
<p> Lecture 2: Random projection and kernels methods. Ensemble methods (boosting and Bagging)
<p> Lecture 3: Unsupervised learning with PCA and Kernel PCA. Spectral clustering
<p> Lecture 4: How to work with	data part 1
<p> Lecture 5: How to work with data part 2
<p> Lecture 6: Introduction to Neural network and deep learning
<p> Lecture 7: Some special architecture: CNN, RNN and LSTM
<p> Lecture 8: Introduction to reinforcement learning


### Where and when?

Lecture will be on monday in April and May 2019, in Ecole Normale Superieure in Paris, in the physics department, Rue Lhomond, at the third floor. The lectures in will be in room  L357 from  9h to 10h30, followed by practical exercices in room L363 and L378.

There will be eight sessions: april 1,8,15,29 and may  6,13,20,27.


### A list of references

<p> A good book for probability and statistics, accessible to students, is Larry A. Wasserman 's <a href="https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf">All of Statistics</a></p>
<p> A good introduction to statistical learning is given in <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of Statistical Learning </a> by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie.</p>
<p> Another great reference is <a href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Machine Learning:A Pro\
babilistic Perspective<a/> by Kevin P. Murphy.</p>
<p> Deep learning is well covered in this new book:
<a href="http://d2l.ai/">Dive into Deep Learning<a/> by A. Zhang, Z. Lipton, M. Li, A.J. Smola. </p>
